#+TITLE: Intro to MPI
#+AUTHOR: Bryce Mazurowski
#+EMAIL: brycepm2@gmail.com
#+OPTIONS: toc:nil

NCSA Advanced Parallel Computing Cohort
Fall 2023

Due date: December 1, 2023

* What is an MPI message and what are its main components? Describe each one of them.
An MPI message is data passed between nodes on a distributed memory
machine. Its main components are the envelope and the body.
- Envelope: Contains the source of the information being sent, the
  destination that the information is being sent to, the communicator
  passing the information, and a tag for the information.
- Body: Contains the buffer which is the data being sent, the data
  type of the message, and the count or size of the message.



* Describe the different between collective and point-to-point (P2P) communication. What are their advantages and disadvantages?
Collective: Send information from one source to several associated
targets. The advantage here is that information travels to all places
at the same time and is shared between all of them and it is typically
faster than P2P communication. Disadvantages are that information
passing is more restrictive. They can also result in deadlocks.

P2P: Send information from one source to one associated target. The
advantages are the message passing is targeted to one source only where
needed. The disadvantage is that information only goes from one point
to another. They can also result in deadlocks.


* Write/find a simple serial program that can easily scale in size.
   1. Define your program’s domain. What are the main variables that could be decomposed to explore parallelism? 
   2. Propose a domain decomposition, describing your goal: do you want to make the program run faster, or process more data? Include a figure if you like.
   3. Based on your domain decomposition, implement your parallelization approach with MPI functions. You may need to use different types of communication.
   4. Discuss your results  

This program performs a linear solve using a gradient-based iterative
solver. It is motivated by basic machine learning applications. We
attempt to solve the linear system:

\[
\bm{A} \bm{x} = \bm{c}
\]

There are a million very capable linear solvers out there but this is
an idea I wanted to try out.

We initialize $\bm{A}$ and $\bm{c}$ as a bunch of random
numbers and inputs to our function graph. $\bm{x}$ are the
parameters we tune with gradient descent. Our loss function $L$ is
taken to be the $L_2$ norm of the residual vector
$\bm{r} = \bm{b} - \bm{A} \tilde{\bm{x}}$

\[
L = \sqrt{\sum_{i=i}^{n_{comp}} r_i r_i}
\]

The chain rule is used to back propagate the loss function to the
parameters $\bm{x}$.

\[
\frac{\partial L}{\partial x_i} =
\frac{\partial L}{\partial r_j} \frac{\partial r_j}{\partial x_i} =
\frac{-1}{L} A_{ji} r_j
\]

Then gradient descent updates $\tilde{\bm{x}}$ as follows:

\[
\tilde{x_i} = \tilde{x_i} - lr * \frac{\partial L}{\partial x_i}
\]

*Insert your serial code here*
#+begin_src cpp
// Iterative linear solver I intend to MPIify
// By Bryce Mazurowski
//

#include <iostream>
#include <vector>
#include <sys/time.h>
#include <math.h>

using std::cout;
using std::endl;
using std::sqrt;
using std::vector;

void matVecProd(const vector<vector<double>>& A,
                const vector<double>& x,
                const double alpha,
                const char trans,
                vector<double>& out);

void vecMinVec(const vector<double>& a,
               const vector<double>& b,
               vector<double>& c);

void dotProd(const vector<double>& a,
             const vector<double>& b,
             double& c);

void gradDescUpdate(const vector<double>& grad,
                    const double learnRate,
                    vector<double>& xNew);

void printVec(const vector<double>& v);

int main() {
  // system size
  int n = 4;
  // A matrix
  vector<vector<double>> A(n, vector<double>(n,0.0));
  // x vector
  vector<double> x(n, 0.0);
  // rhs
  vector<double> b(n, 0.0);
  // residual vector
  vector<double> r(n, 0.0);

  struct timeval sTime, eTime, elapsedTime;
  gettimeofday(&sTime, NULL);
  // random doubles to populate system
  double y, z;
  srand(time(NULL));
  for (int iRow = 0; iRow < n; iRow++) {
    for (int jCol = 0; jCol < n; jCol++) {
      // make random doubles
      y = rand() % RAND_MAX;
      y = float(y) / (RAND_MAX);
      z = rand() % RAND_MAX;
      z = float(z) / (RAND_MAX);
      A[iRow][jCol] = y;
      b[iRow] = z;
    }
  }
  // cout << "b = " << endl;
  // printVec(b); 
  const int nRuns = 5000;
  vector<double> xOut(n, 0.0);
  for (int iPass = 0; iPass <= nRuns; ++iPass) {
    // FORWARD PASS
    // MatrixVector Prod
    matVecProd(A, x, 1.0 /*alpha*/, 'n', xOut);

    // Make residual
    vecMinVec(b, xOut, r);
    // cout << "r = " << endl;
    // printVec(r);

    // ||r||_{2} = \sqrt(r.r)
    double loss = 0.0;
    dotProd(r, r, loss);
    loss = sqrt(loss);
    if (!(iPass % 500)) {
      cout << "Run: " << iPass
           << " LOSS = " << loss << endl;
    }

    // BACKWARD PASS
    // dL/dr_i * dr_i/dx_j = -A_{ji} r_i/loss = -1/loss*a_{ji} r_i
    // dL/dr_i = r_i/loss
    // dr_i/dx_j = -A_{ji}
    vector<double> grad(n, 0.0);
    matVecProd(A, r, -1.0 / loss /*alpha*/, 't', grad);
    // cout << "grad = " << endl;
    // printVec(grad);

    // gradient descent
    double learnRate = 0.001;
    gradDescUpdate(grad, learnRate, x);
    // cout << "x = " << endl;
    // printVec(x);
  }
  // test results
  cout << "x = " << endl;
  printVec(x);
  matVecProd(A, x, 1.0, 'n', xOut);
  vecMinVec(b, xOut, r);
  cout << "Final r = " << endl;
  printVec(r);
  gettimeofday(&eTime, NULL);
  timersub(&eTime, &sTime, &elapsedTime);
  cout << "Elapsed time (s): "  
       << elapsedTime.tv_sec + elapsedTime.tv_usec / 1000000.0 << endl;

  return 0;
}

void matVecProd(const vector<vector<double>>& A,
                const vector<double>& x,
                const double alpha,
                const char trans,
                vector<double>& out) {
  // alpha*A.x
  // trans determines whether A_{ij} or A_{ji}
  // output into nonConst vector
  // zero out
  std::fill(out.begin(), out.end(), 0.0);
  int size = x.size();
  double aComp = 0.0;
  for (int iRow = 0; iRow < size; ++iRow) {
    for (int jCol = 0; jCol < size; ++jCol) {
      if (trans == 'n') {
        aComp = A[iRow][jCol];
      } else {
        aComp = A[jCol][iRow];
      }
      out[iRow] += aComp*x[jCol];
    }
    out[iRow] *= alpha;
  }
}

void vecMinVec(const vector<double>& a,
               const vector<double>& b,
               vector<double>& c) {
  // a_i - b_i = c_i
  for (int iRow = 0; iRow < a.size(); ++iRow) {
    c[iRow] = a[iRow] - b[iRow];
  }
}

void dotProd(const vector<double>& a,
             const vector<double>& b,
             double& c) {
  // a_i . b_i = c
  for (int iRow = 0; iRow < a.size(); ++iRow) {
    c += a[iRow]*b[iRow];
  }
}

void gradDescUpdate(const vector<double>& grad,
                    const double learnRate,
                    vector<double>& xNew) {
  // xNew_i = xNew_i - grad*learnRate
  for (int iRow = 0; iRow < grad.size(); ++iRow) {
    xNew[iRow] -= grad[iRow]*learnRate;
  }
}

void printVec(const vector<double>& v) {
  for (int iRow = 0; iRow < v.size(); ++iRow) {
    std::cout << v[iRow] << " ";
  }
  std::cout << std::endl;
}
#+end_src

*Insert your MPI parallelized code here*
#+begin_src cpp
// Iterative linear solver MPIified
// By Bryce Mazurowski
//

#include <iostream>
#include <vector>
#include <sys/time.h>
#include <cmath>

#include "mpi.h"

#define ORD 512

using std::cout;
using std::endl;
using std::sqrt;
using std::vector;

void matVecProd(const vector<double> A,
                const int chunkSize,
                const vector<double>& x,
                const double alpha,
                const char trans,
                vector<double>& out);

void vecMinVec(const vector<double>& a,
               const vector<double>& b,
               vector<double>& c);

void dotProd(const vector<double>& a,
             const vector<double>& b,
             double& c);

void gradDescUpdate(const vector<double>& grad,
                    const double learnRate,
                    vector<double>& xNew);

void printVec(const vector<double>& v);

int main() {
  int locRank, globRank, mpiErr;
  // local and global loss
  double loss, globLoss;
  // gradient descent rate
  double learnRate = 0.0001;

  // Each threads components
  // A matrix - NOTE: MPI seems to hate vector<vector>
  vector<double> A;
  // x vector - Need full vector in initial split
  vector<double> x;
  vector<double> xTest;
  // work area
  vector<double> xOut;
  // rhs
  vector<double> b;
  // residual vector
  vector<double> r;
  // vector for gradient
  vector<double> grad;
  int rowStart, rowEnd;
  int chunkSize, leftOver;

  // full thread components
  vector<double> fullGrad(ORD);
  vector<double> fullRes(ORD);
  // For testing
  vector<double> globA(ORD*ORD);
  vector<double> globX(ORD);
  vector<double> globB(ORD);

  vector<double> lossCheckVec(ORD);

  // random doubles to populate system
  double y, z;


  double sTime, eTime;
  mpiErr = MPI_Init(NULL, NULL);
  
  // This rank
  MPI_Comm_rank(MPI_COMM_WORLD, &locRank);
  // global rank
  MPI_Comm_size(MPI_COMM_WORLD, &globRank);

  if (locRank == 0) {
    sTime = MPI_Wtime();
  } 
 
  // To start:
  // cut up A, x, b by rowWise
  chunkSize = ORD/globRank;
  leftOver = ORD % globRank;
  if (locRank == (globRank - 1)) {
    chunkSize += leftOver;
  }
  // resize this threads vectors
  A.resize(chunkSize*ORD);
  x.resize(ORD);
  xTest.resize(chunkSize);
  xOut.resize(chunkSize);
  b.resize(chunkSize);
  r.resize(chunkSize);
  grad.resize(chunkSize);

  srand(10);
  if (locRank == 0) {
    for (int iRow = 0; iRow < ORD; ++iRow) {
      for (int jCol = 0; jCol < ORD; ++jCol) {
      // make random doubles
      y = rand() % RAND_MAX;
      y = float(y) / (RAND_MAX);
      globA[iRow*ORD + jCol] = y;
      }
      z = rand() % RAND_MAX;
      z = float(z) / (RAND_MAX);
      globX[iRow] = z;
      x[iRow] = 0.0;
    }
  }

  // each thread fills in their rows
  for (int iRow = 0; iRow < chunkSize; iRow++) {
    // initialize some things
    xOut[iRow] = 0.0;
    r[iRow] = 0.0;
    grad[iRow] = 0.0;
  }
  // Make manufactured solution
  if (locRank == 0) {
    matVecProd(globA, ORD, globX, 1.0 /*alpha*/, 'n', globB);
    // cout << "globA:" << endl;
    // printVec(globA);
    // cout << "globB:" << endl;
    // printVec(globB);
  }
  // Push rhs out to everyone
  mpiErr = MPI_Scatter(globB.data(), chunkSize, MPI_DOUBLE, b.data(),
                       chunkSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  // kick out little slices of globA to each proc
  mpiErr = MPI_Scatter(globA.data(), ORD*chunkSize, MPI_DOUBLE, A.data(),
                      ORD*chunkSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  const int nRuns = 5000;
  for (int iPass = 0; iPass <= nRuns; ++iPass) {
    // FORWARD PASS
    // MatrixVector Prod
    matVecProd(A, chunkSize, x, 1.0 /*alpha*/, 'n', xOut);

    // Make residual
    vecMinVec(b, xOut, r);
    
    // ||r||_{2} = \sqrt(r.r)
    dotProd(r, r, loss);
    mpiErr = MPI_Allreduce(&loss, &globLoss, 1, MPI_DOUBLE,
                           MPI_SUM, MPI_COMM_WORLD);
    globLoss = sqrt(globLoss);
    if (!(iPass % 1000) && locRank == 0) {
      cout << "Run: " << iPass
           << " LOSS = " << globLoss << endl;
    }

    // BACKWARD PASS
    // dL/dr_i * dr_i/dx_j = -A_{ji} r_i/loss = -1/loss*a_{ji} r_i
    // dL/dr_i = r_i/loss
    // dr_i/dx_j = -A_{ji}
    // NOTE: this needs to be global :(
    MPI_Gather(r.data(), chunkSize, MPI_DOUBLE, fullRes.data(),
               chunkSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    if (locRank == 0) {
      matVecProd(globA, ORD, fullRes, -1.0/globLoss /*alpha*/, 't', fullGrad);
      // update x vector
      gradDescUpdate(fullGrad, learnRate, x);
    }
 
    // Update x for all threads
    MPI_Bcast(x.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);
  }

  matVecProd(A, chunkSize, x, 1.0, 'n', xOut);
  vecMinVec(b, xOut, r);
  mpiErr = MPI_Gather(r.data(), chunkSize, MPI_DOUBLE, fullRes.data(),
                      chunkSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  if (locRank == 0) {
    // test results
    // cout << "manufact sol = " << endl;
    // printVec(globX);
    // cout << "final x = " << endl;
    // printVec(x);
    double testLoss = 0.0;
    vecMinVec(globX, x, lossCheckVec);
    dotProd(lossCheckVec, lossCheckVec, testLoss);
    testLoss = sqrt(testLoss);
    cout << "L2 Error = " << testLoss << endl;
    double finLoss = 0.0;
    dotProd(fullRes, fullRes, finLoss);
    finLoss = sqrt(finLoss);
    cout << "Final loss = " << finLoss << endl;
    eTime = MPI_Wtime();
    cout << "Elapsed time (s): "  
         << eTime - sTime << endl;
  }
  mpiErr = MPI_Finalize();
  return 0;
}

void matVecProd(const vector<double> A,
                const int chunkSize,
                const vector<double>& x,
                const double alpha,
                const char trans,
                vector<double>& out) {
  // alpha*A.x
  // trans determines whether A_{ij} or A_{ji}
  // output into nonConst vector
  // zero out
  std::fill(out.begin(), out.end(), 0.0);
  int nRow = chunkSize;
  int nCol = x.size();
  double aComp = 0.0;
  for (int iRow = 0; iRow < nRow; ++iRow) {
    for (int jCol = 0; jCol < nCol; ++jCol) {
      if (trans == 'n') {
        aComp = A[iRow*nCol + jCol];
      } else {
        aComp = A[jCol*nCol + iRow];
      }
      out[iRow] += aComp*x[jCol];
    }
    out[iRow] *= alpha;
  }
}

void vecMinVec(const vector<double>& a,
               const vector<double>& b,
               vector<double>& c) {
  // a_i - b_i = c_i
  for (int iRow = 0; iRow < a.size(); ++iRow) {
    c[iRow] = a[iRow] - b[iRow];
  }
}

void dotProd(const vector<double>& a,
             const vector<double>& b,
             double& c) {
  c = 0.0;
  // a_i . b_i = c
  for (int iRow = 0; iRow < a.size(); ++iRow) {
    c += a[iRow]*b[iRow];
  }
}

void gradDescUpdate(const vector<double>& grad,
                    const double learnRate,
                    vector<double>& xNew) {
  // xNew_i = xNew_i - grad*learnRate
  for (int iRow = 0; iRow < grad.size(); ++iRow) {
    xNew[iRow] -= grad[iRow]*learnRate;
  }
}

void printVec(const vector<double>& v) {
  for (int iRow = 0; iRow < v.size(); ++iRow) {
    std::cout << v[iRow] << " ";
  }
  std::cout << std::endl;
}
#+end_src

*Define your program domain here*

The main variable to decompose is the $n \times n$ matrix
$\bm{A}$. This could be a huge dense matrix that could cause memory
trouble. If we can decompose this it may be possible to solve big
problems on distributed memory machines. This also allows us to
perform linear algebra operations on smaller portions of a matrix
which can be a big time saver.

*Propose your domain decomposition here*

This turned out to be quite a learning experience. Initially I was
hoping to cut the matrix up into small squares based on the global
rank of the program. I settled on breaking up the matrix by clusters
of rows depending on the global rank. This made all of the linear
algebra straightforward. Extending to squares should be doable, but it
would require quite a bit of work under the hood.

*Discuss your results here*

Efficiency ended up being somewhat disappointing. At first, I thought
I could do the gradient calculation on a rank-local matrix and vector
and things were very fast and efficient, but they were not correct. I
fixed this and efficiency plummeted. There is a chance that a more
clever loss function could skyrocket the efficiency. Anyhow, the
program does seem to be progressing toward the solution.

There are a lot of MPI operations within that facilitate the
work. Definitely learned a lot trying this out. I have gained further
evidence that I should leave the linear solver implementations to the
linear solver experts. 

Below are the speedUp results for 5000 iterations of the solver with a
512x512 matrix.
| Threads |    Run1 |    Run2 |    Run3 |    Run4 |       Avg |   Speedup |
|---------+---------+---------+---------+---------+-----------+-----------|
|       1 | 38.4241 | 38.3162 | 38.3086 | 38.2995 |   38.3371 |        1. |
|       2 | 29.6095 | 29.5207 | 29.5345 | 29.5578 | 29.555625 | 1.2971169 |
|       4 | 25.3982 | 25.3976 | 25.3197 | 25.3742 | 25.372425 | 1.5109750 |
|       8 |  23.591 | 23.5758 | 23.5797 | 23.7527 |   23.6248 | 1.6227481 |
|      16 | 23.9134 | 24.2508 | 23.9528 |  23.951 |    24.017 | 1.5962485 |
|      32 |  24.219 |   23.88 |  24.225 | 23.8472 |   24.0428 | 1.5945356 |
#+TBLFM: $6=vsum($2..$5)/4::$7=@2$6/$6
