#+TITLE: 1st Assignment - Intro to Parallel Computing
#+AUTHOR: Bryce Mazurowski
#+EMAIL: brycepm2@illinois.edu

NCSA Advanced Parallel Computing Cohort
Fall 2023

Due date: October 11, 2023


* 1. Describe the two pure ways of parallelism (data and task). Which one of them has the most capability to speed up computationally intensive workloads?
- Data Parallelism: When a single task is executed on a chunk of data
  that is split into pieces based on the number of threads available
  for the task
- Task Parallelism: When multiple independent tasks are executed on a single data
  source. Each task can be assigned a thread and computed
  concurrently.
- In general, data parallelism is more capable of speeding up
  computationally intensive workflows. When working with a large
  dataset, it is quicker to partition the data and work on a given
  task concurrently. Task parallelism is also more limited if one task
  depends on the operations in a different task.

* 2. What do you perceive as the best advantage of OpenMP? What is its main limitation?
The biggest advantage of OpenMP is its widespread availability. It can
be used on any operating system with basically any compiler. The fact
that it is simple to work with is a close second.

OpenMP's biggest limitation is that it is only effective on shared
memory machines. It cannot handle communication on distributed memory systems.



* 3. Write/find a simple serial program with a loop (ask ChatGPT if you can’t find one).
 1. Parallelize the loop using OpenMP directives
 2. Measure the time it takes to complete the loop with a different number of OpenMP threads (1 to 16 suggested)
 3. Repeat b) 3-6 times, take the averages, and calculate the speedup factor for each number of threads
 4. Plot your results
 5. Discuss the scalability of your parallelization. Is it far from ideal? Why?

 Ideas:
 - LaPack operation
 - derivative Appox?
 - Integral Approx?
 - Sorting array

Insert your serial code here
BPMNote: compile command:
#+begin_src shell
  clang++ -o mandelbrotSerial mandelloop.cpp
#+end_src
#+begin_src cpp
  #include <iostream>
  #include <vector>
  #include <complex>
  #include <sys/time.h>

  #define ord 1 << 3

  typedef std::complex<float> cFloat;

  int mandelbrot(const cFloat c) const; 

  int main() {
    // instantiate vector of 
    std::vector<cFloat> v;
    float a, b;
    srand(time(NULL));
    for (int iPos = 0; iPos < ORD; iPos++) {
      // make 2 random doubles
      a = rand() % RAND_MAX;
      a = float(a) / (RAND_MAX);
      b = rand() % RAND_MAX;
      b = float(b) / (RAND_MAX);
      // make our complex number
      cFloat z (a, b);
      // add it to the vector
      v.push_back(z); 
    }

    std::vector<cFloat>::iterator vPos = v.begin();
    std::vector<cFloat>::iterator vEnd = v.end();
    for ( ; vPos != vEnd; vPos++) {
      std::cout << "vPos = " << (*vPos) << std::endl;
      // calculate setVal
      const int iTs = mandelbrot( *vPos);
      std::cout << "iTs = " << iTs << std::endl;
    }
    return 0;
  }

  int mandelbrot(const std::complex<float> c) {
    std::complex<float> z = c;
    for (unsigned int iMan = 0; iMan < 200; iMan++) {
      if ( std::abs(z) > 2.0 ) {
        // outside of magnitude bounds
        return iMan;
      }
      // square z
      z = z*z + c;
    }
    return iMan;
  }

  #+end_src




	

Insert your parallelized code here
#+begin_src cpp

#+end_src




	

Add your plot here


Discuss the results here
