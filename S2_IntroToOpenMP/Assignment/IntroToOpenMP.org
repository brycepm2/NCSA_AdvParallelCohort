#+TITLE: Intro to OpenMP
#+AUTHOR: Bryce Mazurowski
#+EMAIL: brycepm2@gmail.com

NCSA Advanced Parallel Computing Cohort
Fall 2023

Due date: October 25, 2023


Name: Bryce Mazurowski
Email: brycepm2@illinois.edu




* What is loop scheduling? Describe at least two different schedules and discuss what type of workloads can benefit from each.
Loop scheduling describes different strategies to partition a given
dataset used in a data parallelization framework. There are three main
loop scheduling strategies: static, dynamic, and guided.
- Static: full dataset is cut into equal chunks based on the number of
  threads available to OpenMP. This is the default behavior and good
  for situations where workload is relatively constant for any index
- Dynamic: dataset is cut into chunks of size $R$, in principle this
  should lead to more chunks than there are threads available. Each
  thread is assigned a chunk, and when it finishes the task on a given
  chunk it a new one is assigned to it. This strategy can be useful
  when the workload of a task depends on the index of the loop.
- Guided: dataset is cut into equal portions and tasks are
  started. Once a thread finishes, the remaining data is redistributed
  equally again among all threads. This continues until the
  redistribution pieces are a minimum size $R$. This strategy is
  useful for cases where the workload increases with index.

* What is a reduction clause? Why are they useful? Give a simple example of a situation where using a reduction clause would be appropriate.
Reduction clauses are instructions on how to combine data
members in a parallel section. They give each thread a private copy of
a variable that it can operate and and tell the program how each
thread's pieces fit together in the end. These greatly simplify
implementation and can avoid omp barrier that will slow down code.

An example where a reduction clause is appropriate would be computing
the maximum element of a large vector. Using a reduction clause on a
private variable max would allow each chunk to compute its own max and
then perform a final comparison once all threads are complete to get
one final value out.

* Write/find a simple serial program with a race condition or a nested loop (ask ChatGPT if you can’t find one).
   1. Parallelize the loop using OpenMP directives without explicitly declaring data contexts
   2. Check to see if your code is doing what it was supposed to do (it should not if you do have a race condition)
   3. Use data contexts (shared and private) to resolve the race condition and check to see if you get the expected results
   4. Discuss your results  

This code has nested loops and race conditions. The code loops over a
vector of ints. In each iteration of the top loop, a second loop
occurs over a set number of points. In the second loop, a value
is assigned to a dataSet for each iteration. On inner loop exit, the data
structure is inserted into a std::map as a pair of outer loop int and
data structure. This is to mimic a specific problem I have faced in
numerical integration of nonlinear solid mechanics problems.

The nested loop requires a private variable or a race condition will
occur.

The map insert will intermittently crash the code in parallel because
one thread will try to insert in the map when another is resorting it
on insert. This is a tricky bug that a reduction clause should fix.

Insert your serial code here
#+begin_src cpp 

#+end_src

Insert your parallelized code without explicit data contexts here
#+begin_src cpp 

#+end_src

Insert your final parallelized code here (race condition resolved)
#+begin_src cpp 

#+end_src

Discuss your results here

This code encounters 2 error categories:
1) If the nested iterator ~iPt~ is not declared private, the code can
   skip points to add to the map. In practice this does not show
   up, since the iterator is declared within the parallel region and
   OpenMP automatically makes anything in there private.
2) If the map ~svMap~ is not private, each thread shares it. Sometimes
   this results in a segmentation fault because the code is trying to
   insert into the map while another may be reSorting it. Sometimes
   this results in a element not being added to the map, so ~map.size()~
   ends up incorrect.

To fix issue (2) I can't just make svMap private or it will always
return a map with zero size.
#+begin_src cpp 
pragma omp for private(svMap)
Total time (s): 0.000262022
map length: 0 correct is 16
#+end_src
I can do lastprivate, but that is still incorrect. That just takes the
value from the last thread to finish, not the complete thing.
#+begin_src cpp 
pragma omp for lastprivate(svMap)
Total time (s): 0.000140905
map length: 2 correct is 16
#+end_src
What I need is a reduction clause that will merge all of the maps.
#+begin_src cpp 
pragma omp for reduction(mapMerge:svMap)
Total time (s): 0.0644159
map length: 32768 correct is 32768
#+end_src

This was quite interesting. Implementing the reduction clause for the
map was a cool experience and this is a real problem I hit in
scientific computing. I had a better solution for that particular
problem, but was curious if OpenMP alone could do it without critical
sections. It seems that it indeed can.

I did not strictly use a data context, but reduction clauses imply
private variables for each thread. The reduction clause is just a
special case of a private variable.

Test speedUp and comment on it!!
